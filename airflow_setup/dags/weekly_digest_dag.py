"""
Weekly Market Digest DAG
Schedule: 0 8 * * 1 (every Monday 08:00 UTC â€” covers previous Mon~Sun)

Generates a comprehensive Korean weekly summary covering:
  - US macroeconomic conditions (Fed, rates, CPI, GDP, jobs)
  - Major stock index performance
  - Key earnings reports and corporate news
  - Sector highlights (energy/nuclear, AI/semis, infrastructure)
  - Global macro risks
  - Next-week calendar
"""

import logging
import os
from datetime import datetime, timedelta
from urllib.parse import quote_plus

import feedparser
import pandas as pd
from sqlalchemy import text

from airflow import DAG
from airflow.operators.python import PythonOperator

from common import DEFAULT_ARGS, get_engine, make_neon_sync_task

log = logging.getLogger(__name__)

SYMBOL_NAMES = {
    "AVGO": "Broadcom", "BE": "Bloom Energy", "VRT": "Vertiv",
    "SMR": "NuScale Power", "OKLO": "Oklo", "GEV": "GE Vernova",
    "MRVL": "Marvell Technology", "COHR": "Coherent Corp", "LITE": "Lumentum",
    "VST": "Vistra Energy", "ETN": "Eaton Corporation",
    "267260.KS": "HDí˜„ëŒ€ì¼ë ‰íŠ¸ë¦­", "034020.KS": "ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°",
    "028260.KS": "ì‚¼ì„±ë¬¼ì‚°", "267270.KS": "HDí˜„ëŒ€ì¤‘ê³µì—…", "010120.KS": "LS ELECTRIC",
    "SBGSY": "Schneider Electric", "HTHIY": "Hitachi",
}

# Google News RSS queries covering broader market context
MARKET_FEEDS = [
    ("ë¯¸êµ­ ê²½ì œ & ì—°ì¤€",    "Federal Reserve interest rates CPI inflation GDP US economy"),
    ("ì£¼ì‹ì‹œìž¥ ë™í–¥",        "S&P 500 NASDAQ Dow Jones stock market weekly"),
    ("ê¸°ì—… ì‹¤ì  ë°œí‘œ",       "earnings report quarterly results EPS revenue beat miss"),
    ("AI & ë°˜ë„ì²´",          "AI semiconductor Broadcom Marvell Nvidia chip demand"),
    ("ì—ë„ˆì§€ & ì›ìžë ¥",      "nuclear energy SMR power grid electricity utility"),
    ("ê¸€ë¡œë²Œ & ì§€ì •í•™",      "global economy trade tariff China geopolitics currency"),
]
GNEWS_URL = "https://news.google.com/rss/search?q={q}&hl=en-US&gl=US&ceid=US:en"


# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _week_range() -> tuple:
    """Return (week_start, week_end) as date objects for the previous 7 days."""
    today = datetime.utcnow().date()
    week_end   = today - timedelta(days=1)
    week_start = week_end - timedelta(days=6)
    return week_start, week_end


def _rss_headlines(query: str, n: int = 8) -> list[str]:
    try:
        feed = feedparser.parse(GNEWS_URL.format(q=quote_plus(query)))
        return [e.title for e in feed.entries[:n]]
    except Exception as exc:
        log.warning("RSS fetch failed (%s): %s", query[:30], exc)
        return []


# â”€â”€ Task functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _collect_digest_data(week_start, week_end) -> dict:
    """Collect all data needed for digest (DB prices, DB news, RSS headlines)."""
    with get_engine().connect() as conn:
        price_df = pd.read_sql(
            text("""
                SELECT symbol, trade_date, close
                FROM stock_prices
                WHERE trade_date BETWEEN :s AND :e
                ORDER BY symbol, trade_date
            """),
            conn, params={"s": week_start, "e": week_end},
        )
        news_df = pd.read_sql(
            text("""
                SELECT symbol, title, published
                FROM stock_news
                WHERE published >= :s
                  AND published <  :e + INTERVAL '1 day'
                ORDER BY published DESC
                LIMIT 80
            """),
            conn, params={"s": week_start, "e": week_end},
        )

    # Weekly return per symbol
    returns = {}
    for sym, grp in price_df.groupby("symbol"):
        grp = grp.sort_values("trade_date")
        if len(grp) >= 2:
            returns[sym] = (grp["close"].iloc[-1] / grp["close"].iloc[0] - 1) * 100

    # RSS headlines per topic
    rss_data = {}
    for topic, query in MARKET_FEEDS:
        headlines = _rss_headlines(query, n=7)
        if headlines:
            rss_data[topic] = headlines

    return {"returns": returns, "news_df": news_df, "rss_data": rss_data}


def _build_basic_digest(week_start, week_end, data: dict) -> str:
    """Build a template-based digest (no AI) from collected data."""
    returns  = data["returns"]
    news_df  = data["news_df"]
    rss_data = data["rss_data"]

    lines = []

    # â”€â”€ ì„¹ì…˜ 1: ì¶”ì  ì¢…ëª© ì£¼ê°„ ìˆ˜ìµë¥  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lines.append("## ðŸ“Š ì´ë²ˆ ì£¼ ì¶”ì  ì¢…ëª© ìˆ˜ìµë¥ \n")
    if returns:
        sorted_rets = sorted(returns.items(), key=lambda x: x[1], reverse=True)
        gainers = [(s, r) for s, r in sorted_rets if r > 0]
        losers  = [(s, r) for s, r in sorted_rets if r <= 0]

        lines.append("**ìƒìŠ¹ ì¢…ëª©**")
        if gainers:
            for sym, ret in gainers:
                name = SYMBOL_NAMES.get(sym, sym)
                lines.append(f"- {sym} ({name}): **{ret:+.1f}%**")
        else:
            lines.append("- ì—†ìŒ")

        lines.append("\n**í•˜ë½ ì¢…ëª©**")
        if losers:
            for sym, ret in losers:
                name = SYMBOL_NAMES.get(sym, sym)
                lines.append(f"- {sym} ({name}): **{ret:+.1f}%**")
        else:
            lines.append("- ì—†ìŒ")
    else:
        lines.append("_ì´ë²ˆ ì£¼ ê°€ê²© ë°ì´í„° ì—†ìŒ (stock_price_collection DAG ì‹¤í–‰ í•„ìš”)_")

    lines.append("")

    # â”€â”€ ì„¹ì…˜ 2~6: RSS ë‰´ìŠ¤ í—¤ë“œë¼ì¸ (topicë³„) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    SECTION_ICONS = {
        "ë¯¸êµ­ ê²½ì œ & ì—°ì¤€":  "ðŸ¦ ê±°ì‹œê²½ì œ & ì—°ì¤€ ë™í–¥",
        "ì£¼ì‹ì‹œìž¥ ë™í–¥":      "ðŸ“ˆ ì£¼ì‹ì‹œìž¥ ë™í–¥",
        "ê¸°ì—… ì‹¤ì  ë°œí‘œ":     "ðŸ“‹ ì£¼ìš” ì‹¤ì  ë°œí‘œ",
        "AI & ë°˜ë„ì²´":        "âš¡ AI & ë°˜ë„ì²´",
        "ì—ë„ˆì§€ & ì›ìžë ¥":    "âš¡ ì—ë„ˆì§€ & ì›ìžë ¥",
        "ê¸€ë¡œë²Œ & ì§€ì •í•™":    "ðŸŒ ê¸€ë¡œë²Œ & ì§€ì •í•™",
    }
    for topic, _ in MARKET_FEEDS:
        headlines = rss_data.get(topic, [])
        section_title = SECTION_ICONS.get(topic, topic)
        lines.append(f"## {section_title}\n")
        if headlines:
            for h in headlines:
                lines.append(f"- {h}")
        else:
            lines.append("_í—¤ë“œë¼ì¸ ìˆ˜ì§‘ ì‹¤íŒ¨_")
        lines.append("")

    # â”€â”€ ì„¹ì…˜ 7: ì¶”ì  ì¢…ëª© ë‰´ìŠ¤ í—¤ë“œë¼ì¸ (DB) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lines.append("## ðŸ“° ì¶”ì  ì¢…ëª© ì£¼ê°„ ë‰´ìŠ¤\n")
    if not news_df.empty:
        for _, row in news_df.head(30).iterrows():
            sym  = row["symbol"]
            name = SYMBOL_NAMES.get(sym, sym)
            lines.append(f"- **[{sym}]** {row['title']}")
    else:
        lines.append("_ì´ë²ˆ ì£¼ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì—†ìŒ_")
    lines.append("")

    # â”€â”€ ì•ˆë‚´ ë©”ì‹œì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lines.append("---")
    lines.append(
        "> â„¹ï¸ **ê¸°ë³¸ ëª¨ë“œ**: ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ AI ë¶„ì„ ì—†ì´ "
        "ì›ë³¸ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ í‘œì‹œí•©ë‹ˆë‹¤. API í‚¤ë¥¼ `.env`ì— ì¶”ê°€í•˜ë©´ "
        "Claude AIê°€ ì‹¬ì¸µ ë¶„ì„Â·ìš”ì•½Â·ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•œ ì „ë¬¸ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
    )

    return "\n".join(lines)


def generate_weekly_digest():
    """
    1) Pull tracked-symbol weekly returns + news from DB
    2) Fetch general market headlines via Google News RSS
    3) Call Claude Sonnet to write a comprehensive Korean digest
       (fallback: template-based digest when API key not set)
    4) Upsert result into weekly_digest table
    """
    week_start, week_end = _week_range()
    log.info("Generating weekly digest for %s ~ %s", week_start, week_end)

    data = _collect_digest_data(week_start, week_end)

    api_key = os.environ.get("ANTHROPIC_API_KEY", "")
    ai_available = bool(api_key)
    if ai_available:
        try:
            import anthropic
        except ImportError:
            log.warning("anthropic package not installed â€” using basic digest")
            ai_available = False

    if ai_available:
        # â”€â”€ AI digest via Claude Sonnet â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        returns  = data["returns"]
        news_df  = data["news_df"]
        rss_data = data["rss_data"]

        price_block = "ë°ì´í„° ì—†ìŒ"
        if returns:
            sorted_rets = sorted(returns.items(), key=lambda x: x[1], reverse=True)
            price_block = "\n".join(
                f"  {sym:12s} ({SYMBOL_NAMES.get(sym, sym):20s})  {ret:+.1f}%"
                for sym, ret in sorted_rets
            )

        news_block = "ì—†ìŒ"
        if not news_df.empty:
            lines = []
            for _, row in news_df.head(40).iterrows():
                sym  = row["symbol"]
                name = SYMBOL_NAMES.get(sym, sym)
                lines.append(f"  [{sym}/{name}] {row['title']}")
            news_block = "\n".join(lines)

        rss_block = ""
        for topic, headlines in rss_data.items():
            rss_block += f"\n[{topic}]\n" + "\n".join(f"  - {h}" for h in headlines) + "\n"
        if not rss_block:
            rss_block = "RSS ìˆ˜ì§‘ ì‹¤íŒ¨"

        prompt = f"""ë„ˆëŠ” ë¯¸êµ­Â·ê¸€ë¡œë²Œ ì£¼ì‹ì‹œìž¥ê³¼ ê²½ì œë¥¼ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ì‹œë‹ˆì–´ ì• ë„ë¦¬ìŠ¤íŠ¸ì•¼.
ì•„ëž˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ë²ˆ ì£¼({week_start} ~ {week_end}) ì£¼ê°„ ì‹œìž¥ ì´ìŠˆ ëª¨ìŒì„ í•œêµ­ì–´ë¡œ ìž‘ì„±í•´ì¤˜.
íˆ¬ìžìžê°€ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìžˆë„ë¡ í•µì‹¬ë§Œ ëª…í™•í•˜ê²Œ, ì¸ì‚¬ì´íŠ¸ ìžˆê²Œ ì¨ì¤˜.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¶ ì¶”ì  ì¢…ëª© ì£¼ê°„ ìˆ˜ìµë¥ 
{price_block}

â–¶ ì´ë²ˆ ì£¼ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ (DB)
{news_block}

â–¶ ì‹œìž¥ ì „ë°˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ (Google News RSS, ì˜ë¬¸)
{rss_block}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ì•„ëž˜ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ìž‘ì„±í•´ì¤˜. ê° ì„¹ì…˜ì€ ì¶©ë¶„ížˆ êµ¬ì²´ì ìœ¼ë¡œ(3~6ê°œ í¬ì¸íŠ¸):

## ðŸ“Š ì´ë²ˆ ì£¼ ì‹œìž¥ í•œëˆˆì— ë³´ê¸°
(ì „ë°˜ì ì¸ ì‹œìž¥ ë¶„ìœ„ê¸°Â·ì£¼ìš” ì§€ìˆ˜ íë¦„Â·íˆ¬ìž ì‹¬ë¦¬ ìš”ì•½ 3~4ì¤„)

## ðŸ¦ ê±°ì‹œê²½ì œ & ì—°ì¤€ ë™í–¥
(ê¸ˆë¦¬Â·ì¸í”Œë ˆì´ì…˜Â·GDPÂ·ê³ ìš© ê´€ë ¨ ì£¼ìš” ì´ìŠˆ, ë¶ˆë¦¿ í¬ì¸íŠ¸)

## ðŸ“ˆ ì£¼ìš” ì‹¤ì  ë°œí‘œ & ê¸°ì—… ì´ìŠˆ
(ì´ë²ˆ ì£¼ ì£¼ìš” ì‹¤ì Â·ì–´ë‹ ì„œí”„ë¼ì´ì¦ˆÂ·ê¸°ì—…ë³„ ì´ìŠˆ, ì¶”ì  ì¢…ëª© ì–¸ê¸‰ í¬í•¨)

## âš¡ ì„¹í„°ë³„ í•µì‹¬ ì´ìŠˆ
(AIÂ·ë°˜ë„ì²´, ì—ë„ˆì§€Â·ì›ìžë ¥Â·ì „ë ¥ì¸í”„ë¼, ì‚°ì—…ìž¬ ë“± ì„¹í„°ë³„ í•˜ì´ë¼ì´íŠ¸)

## ðŸŒ ê¸€ë¡œë²Œ & ê±°ì‹œ ë¦¬ìŠ¤í¬
(ì§€ì •í•™, ë¬´ì—­, ë‹¬ëŸ¬Â·ì›í™”, ì¤‘êµ­Â·ìœ ëŸ½ ë“± ê¸€ë¡œë²Œ ë³€ìˆ˜)

## ðŸ“… ë‹¤ìŒ ì£¼ ì£¼ëª© ì´ë²¤íŠ¸
(ì˜ˆì •ëœ ê²½ì œì§€í‘œ ë°œí‘œ, ì‹¤ì  ë°œí‘œ, FOMCÂ·ì¤‘ì•™ì€í–‰ ì¼ì •, ì£¼ëª©í•  ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸)

## ðŸ’¡ ì´ë²ˆ ì£¼ í•µì‹¬ í•œ ì¤„ ìš”ì•½
(ì „ì²´ë¥¼ í•œ ë¬¸ìž¥ìœ¼ë¡œ ì••ì¶•)"""

        response = anthropic.Anthropic(api_key=api_key).messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=2500,
            messages=[{"role": "user", "content": prompt}],
        )
        content = response.content[0].text.strip()
        log.info("AI digest generated via Claude Sonnet")

    else:
        # â”€â”€ Fallback: template-based digest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        log.info("ANTHROPIC_API_KEY not set â€” building basic template digest")
        content = _build_basic_digest(week_start, week_end, data)

    headline = (
        f"{week_start.strftime('%Yë…„ %mì›” %dì¼')} ~ "
        f"{week_end.strftime('%mì›” %dì¼')} ì£¼ê°„ ì‹œìž¥ ì´ìŠˆ"
    )

    # â”€â”€ Upsert into DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with get_engine().begin() as conn:
        conn.execute(
            text("""
                INSERT INTO weekly_digest (week_start, week_end, headline, content)
                VALUES (:week_start, :week_end, :headline, :content)
                ON CONFLICT (week_start) DO UPDATE SET
                    headline   = EXCLUDED.headline,
                    content    = EXCLUDED.content,
                    updated_at = NOW()
            """),
            {
                "week_start": week_start,
                "week_end":   week_end,
                "headline":   headline,
                "content":    content,
            },
        )
    log.info("Weekly digest saved: %s", headline)


def _digest_complete():
    log.info("weekly_digest DAG finished successfully")


# â”€â”€ DAG definition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_DIGEST_ARGS = {**DEFAULT_ARGS, "retries": 1, "retry_delay": timedelta(minutes=10)}

with DAG(
    dag_id="weekly_digest",
    default_args=_DIGEST_ARGS,
    schedule_interval="0 8 * * 1",   # Every Monday 08:00 UTC
    start_date=datetime(2024, 1, 1),
    catchup=False,
    max_active_runs=1,
    tags=["stocks", "digest", "weekly"],
    doc_md=__doc__,
) as dag:

    generate = PythonOperator(
        task_id="generate_weekly_digest",
        python_callable=generate_weekly_digest,
    )

    complete = PythonOperator(
        task_id="digest_complete",
        python_callable=_digest_complete,
    )

    sync_neon = PythonOperator(
        task_id="sync_to_neon",
        python_callable=make_neon_sync_task(["weekly_digest"]),
    )

    generate >> complete >> sync_neon
